{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f9fdef-226b-4d44-966a-3715901e2625",
   "metadata": {},
   "source": [
    "# **Blood Transfusion Service Center Dataset Preprocessing and Splitting**\r\n",
    "\r\n",
    "In this tutorial, we will preprocess the **Blood Transfusion Service Center Dataset** from the UCI Machine Learning Repository. We will normalize the features, encode the target variable, and split the dataset into training, validation, and test sets.\r\n",
    "\r\n",
    "-\n",
    "## **Step 1: Import Necessary Libries**\r\n",
    "\r\n",
    "We start by importing the required libraries, including `pandas` for data manipulation, `train_test_split` from `sklearn.model_selection` to split the dataset, `MinMaxScaler` for normalization, and `LabelEncoder` for encoding categorical  fetch_ucirepo\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f9b68-a821-4422-9705-a676ade4af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from ucimlrepo import fetch_ucirepo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08a652-d1e4-4105-84c3-87978e9d6e5c",
   "metadata": {},
   "source": [
    "Step 2: Fetch the Dataset\n",
    "We fetch the Blood Transfusion Service Center Dataset using the fetch_ucirepo function from the UCI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44780112-dede-4c04-a30f-53f1fe8a41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset from UCI repository\n",
    "blood_transfusion_service_center = fetch_ucirepo(id=176)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145dc58-8e84-42db-9d18-106e0e5c9ce1",
   "metadata": {},
   "source": [
    "Step 3: Extract Features and Target\n",
    "The dataset is extracted into a pandas DataFrame, where X contains the input features and y contains the target variable indicating whether a person donated blood or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df61e5-a0d2-4110-b4fe-d528d3c586a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data as pandas DataFrame\n",
    "X = blood_transfusion_service_center.data.features\n",
    "y = blood_transfusion_service_center.data.targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f959-04df-431d-ac9b-fc8c6d51ae3b",
   "metadata": {},
   "source": [
    "Step 4: Display the First Few Rows\n",
    "We display the first few rows of the dataset to understand its structure and see the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917c819-47af-4bd4-aa24-6061a7ebd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\\n\", X.head())\n",
    "print(\"Target variable (Class):\\n\", y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb423e3-2a6a-4192-8216-1c9347553006",
   "metadata": {},
   "source": [
    "Step 5: Encode the Target Variable\n",
    "If the target variable y contains categorical values such as 'yes'/'no', we use LabelEncoder to convert these to numerical values (0 and 1) for easier processing in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7b52c-7795-4eee-b0e9-a4dd426d9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable (if not already binary numerically encoded)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f9158-3b00-44d7-a382-6522a2e9f6a5",
   "metadata": {},
   "source": [
    "Step 6: Normalize the Features\n",
    "We apply Min-Max Scaling to normalize the features. This scales the feature values to a range of [0, 1], which can improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b6415-f43d-4301-a471-07be4952b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b214a-cb85-466b-b15f-e845f054a5c8",
   "metadata": {},
   "source": [
    "Step 7: Convert Normalized Features Back to DataFrame\n",
    "After normalization, we convert the normalized features back to a DataFrame for easier manipulation and to retain the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593a169-8ef4-42a1-a6b7-4be6a071d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the normalized features back to a DataFrame for easier manipulation\n",
    "df_normalized = pd.DataFrame(X_normalized, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e69b9-2dfe-4b3a-8717-552926933548",
   "metadata": {},
   "source": [
    "Step 8: Split the Dataset\n",
    "We split the normalized dataset into training (70%), validation (15%), and test (15%) sets. First, we split the dataset into training and a temporary set, and then further split the temporary set into validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07bfdc-f99c-401a-8def-5553bacfa3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the normalized dataset into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df_normalized, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the temporary set into validation (15%) and test (15%) sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85838e80-d204-46b3-be55-eab0e8d9011a",
   "metadata": {},
   "source": [
    "Step 9: Print the Size of Each Split\n",
    "We print the size of each dataset split to verify that the proportions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12212d7c-cac9-44eb-9017-4941d3397af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of each split\n",
    "print(\"\\nTraining set size: \", X_train.shape)\n",
    "print(\"Validation set size: \", X_val.shape)\n",
    "print(\"Test set size: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c45a7-5637-421f-9404-68ae1a036ead",
   "metadata": {},
   "source": [
    "Step 10: Optional Sample Verification\n",
    "As an optional step, we can print a few samples from each split to verify the process and ensure that the data has been correctly divided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834f166-5aa2-4068-9477-44ecbc0dd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print a few samples from each split to verify the process\n",
    "print(\"\\nFirst few training samples:\\n\", X_train.head())\n",
    "print(\"\\nFirst few validation samples:\\n\", X_val.head())\n",
    "print(\"\\nFirst few test samples:\\n\", X_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e13735-253c-4e7f-89af-9fcb41c47529",
   "metadata": {},
   "source": [
    "Step 11: Optional Class Distribution Check\n",
    "Lastly, we can check the class distribution in the training set to ensure that the encoding has been performed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cfbb3-dc69-42c8-bbee-a536d0e2577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print the class distribution to ensure correct encoding\n",
    "print(\"\\nEncoded target distribution in training set:\", pd.Series(y_train).value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
